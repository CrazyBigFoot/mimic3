{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic modules\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "import itertools\n",
    "from pprint import pprint\n",
    "\n",
    "# scikit-learn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, fbeta_score \n",
    "from sklearn.metrics import roc_curve, roc_auc_score, make_scorer\n",
    "from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "from sklearn.model_selection import StratifiedKFold, KFold, RepeatedStratifiedKFold, cross_val_score\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "\n",
    "# models\n",
    "import xgboost as xgb\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier, StackingClassifier\n",
    "\n",
    "#settings\n",
    "warnings.filterwarnings('ignore')\n",
    "rs = 42\n",
    "fhalf_scorer = make_scorer(fbeta_score, beta=0.5)\n",
    "c = StratifiedKFold(n_splits=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./data/mimic_premodel.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scaling and Train/Test-Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.iloc[:, 1:]\n",
    "y = df.label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=rs, stratify=y)\n",
    "\n",
    "# Show the results of the split\n",
    "print (\"Training set has {} samples.\".format(X_train.shape[0]))\n",
    "print (\"Testing set has {} samples.\".format(X_test.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = preprocessing.StandardScaler().fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "source": [
    "# Functions"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dummy Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dumm = DummyClassifier(strategy= \"stratified\", random_state=rs)\n",
    "dumm.fit(X_train, y_train)\n",
    "y_pred_dumm = dumm.predict(X_test)\n",
    "\n",
    "cnf_matrix = confusion_matrix(y_test, y_pred_dumm)\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "print(classification_report(y_test, y_pred_dumm))\n",
    "print(f'F-Beta-Score(0.5) =  {fbeta_score(y_test, y_pred_dumm, beta=0.5 ):.2f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot non-normalized confusion matrix\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix, classes=['Failed','Succeeded'],\n",
    "                      normalize= False,  title='Confusion matrix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot the ROC curve\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred_dumm)\n",
    "plt.plot(fpr, tpr, linestyle='--',label=\"Dummy\") \n",
    "plt.legend()\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.show()\n",
    "y_pred_dumm_prob = dumm.predict_proba(X_test)\n",
    "print(f\"AUC(Dummy) = {roc_auc_score(y_test, y_pred_dumm_prob[:,1]):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Base Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "zuerst Standardeinstellungen f√ºr die Hyperparameter, danach Hyperparameter-Tuning (Random- und Grid-Search)\n",
    "\n",
    "logistic regression (Jacqueline)  \n",
    "decision tree (Nina)   \n",
    "random forest (Nina)  \n",
    "XGBoost (Niko)  \n",
    "AdaBoost (Niko)  \n",
    "SVM (Mirko)   \n",
    "kNN (Mirko)   \n",
    "Naive Bayes (Jacqueline) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter by default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(random_state=rs)\n",
    "lr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_lr = lr.predict(X_test)\n",
    "print(classification_report(y_test, y_pred_lr))\n",
    "print(f'F-Beta-Score(0.5) =  {fbeta_score(y_test, y_pred_lr, beta=0.5 ):.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot non-normalized confusion matrix\n",
    "cnf_matrix = confusion_matrix(y_test, y_pred_lr)\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix, classes=['Failed','Succeeded'],\n",
    "                      normalize= False,  title='Confusion matrix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the ROC curve\n",
    "fpr_lr, tpr_lr, thresholds_lr = roc_curve(y_test, y_pred_lr)\n",
    "plt.plot(fpr, tpr, linestyle='--', label='Dummy')\n",
    "plt.plot(fpr_lr, tpr_lr, linestyle='-', label='Logistic Regression') \n",
    "plt.legend()\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.show()\n",
    "y_pred_lr_prob = lr.predict_proba(X_test)\n",
    "print(f\"AUC(Dummy) = {roc_auc_score(y_test, y_pred_dumm_prob[:,1]):.2f}\")\n",
    "print(f\"AUC(logReg)= {roc_auc_score(y_test, y_pred_lr_prob[:,1]):.2f}\")"
   ]
  },
  {
   "source": [
    "### Hyperparameter tuning"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter grid for RandomSearch for Logistic Regression\n",
    "param_grid = {\n",
    "    'penalty': [\"l1\", \"l2\", \"elasticnet\", \"none\"],\n",
    "    'C': [100, 10, 1, 0.1, 0.01] \n",
    "}\n",
    "\n",
    "# Estimator for use in random search\n",
    "estimator = LogisticRegression(random_state = rs)\n",
    "\n",
    "# Create the random search model\n",
    "lr_rs = RandomizedSearchCV(estimator, param_grid, n_jobs = -1, \n",
    "                        scoring = fhalf_scorer, cv = c, \n",
    "                        n_iter = 100, verbose = 1, random_state=rs)\n",
    "\n",
    "# Fit \n",
    "lr_rs.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_rs.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_lr_rs = lr_rs.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter grid for GridSearch for Logistic Regression\n",
    "param_grid = {\n",
    "    'penalty': [\"l1\", \"l2\", \"elasticnet\", \"none\"],\n",
    "    'C': [5, 4, 3, 2, 1 , 0.9, 0.8, 0.7, 0.6, 0.5, 0.4, 0.3, 0.2, 0.1, 0.09, 0.08] \n",
    "}\n",
    "\n",
    "# Estimator for use in random search\n",
    "estimator = LogisticRegression(random_state = rs)\n",
    "\n",
    "# Create the random search model\n",
    "lr_gs = GridSearchCV(estimator, param_grid, scoring= fhalf_scorer, cv = c,\n",
    "                     n_jobs = -1, verbose = 1)\n",
    "\n",
    "# Fit \n",
    "lr_gs.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_gs.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_lr_gs = lr_gs.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_lr_best = best_model_lr_gs.predict(X_test)\n",
    "print(classification_report(y_test, y_pred_lr_best))\n",
    "print(f'F-Beta-Score(0.5) =  {fbeta_score(y_test, y_pred_lr_best, beta=0.5 ):.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot non-normalized confusion matrix\n",
    "cnf_matrix = confusion_matrix(y_test, y_pred_lr_best)\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix, classes=['Failed','Succeeded'],\n",
    "                      normalize= False,  title='Confusion matrix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the ROC curve\n",
    "fpr_lr_best, tpr_lr_best, thresholds_lr_best = roc_curve(y_test, y_pred_lr_best)\n",
    "plt.plot(fpr, tpr, linestyle='--', label='Dummy')\n",
    "plt.plot(fpr_lr, tpr_lr, linestyle='-', label='Logistic Regression') \n",
    "plt.plot(fpr_lr_best, tpr_lr_best, linestyle=':', label='Best Logistic Regression')\n",
    "plt.legend()\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.show()\n",
    "y_pred_lr_best_prob = best_model_lr_gs.predict_proba(X_test)\n",
    "print(f\"AUC(Dummy) = {roc_auc_score(y_test, y_pred_dumm_prob[:,1]):.2f}\")\n",
    "print(f\"AUC(logReg)= {roc_auc_score(y_test, y_pred_lr_prob[:,1]):.2f}\")\n",
    "print(f\"AUC(BestlogReg)= {roc_auc_score(y_test, y_pred_lr_best_prob[:,1]):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtree_clf_base = DecisionTreeClassifier(random_state=rs)\n",
    "dtree_clf_base.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y_pred_dtree_clf_base = dtree_clf_base.predict(X_test)"
   ]
  },
  {
   "source": [
    "#### Evaluate Base Model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_pred_dtree_clf_base))\n",
    "print(confusion_matrix(y_test, y_pred_dtree_clf_base))\n",
    "print(f'F-Beta-Score(0.5) =  {fbeta_score(y_test, y_pred_dtree_clf_base, beta=0.5 ):.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC Curve Base Model and Dummy\n",
    "fpr_dt_bm, tpr_dt_bm, thresholds_dt_bm = roc_curve(y_test, y_pred_dtree_clf_base)\n",
    "plt.plot(fpr, tpr, linestyle='--', label='Dummy')\n",
    "plt.plot(fpr_dt_bm, tpr_dt_bm, linestyle='-', label='DT Base Model')\n",
    "plt.legend()\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.show()\n",
    "\n",
    "y_proba_dtree_clf_base = dtree_clf_base.predict_proba(X_test)\n",
    "print(f\"AUC(Dummy) = {roc_auc_score(y_test, y_pred_dumm_prob[:,1]):.2f}\")\n",
    "print(f\"AUC(BaseModel)= {roc_auc_score(y_test, y_proba_dtree_clf_base[:,1]):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at parameters used by our current decision tree\n",
    "print('Parameters currently in use:\\n')\n",
    "pprint(dtree_clf_base.get_params())"
   ]
  },
  {
   "source": [
    "### Random Search"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set scorer for random search to f-beta 0.5\n",
    "fhalf_scorer = make_scorer(fbeta_score, beta=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set cross-validation to a stratified split of 3\n",
    "c_strat=StratifiedKFold(n_splits=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maximum number of levels in tree\n",
    "max_depth=[int(x) for x in np.linspace(10, 110, num = 11)]\n",
    "\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split=[2, 5, 10, 20]\n",
    "\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf=[1, 2, 4, 6, 8]\n",
    "\n",
    "# Number of features to consider at every split\n",
    "max_features=['auto', 'sqrt']\n",
    "\n",
    "# Create the random grid\n",
    "random_grid_dt = {\n",
    "               'max_depth': max_depth,\n",
    "               'max_features': max_features,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf\n",
    "                }\n",
    "pprint(random_grid)\n",
    "#'max_depth': max_depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random search of parameters, using 3 fold cross validation \n",
    "dt_random = RandomizedSearchCV(estimator = dtree_clf_base, param_distributions = random_grid_dt, \n",
    "                               scoring=fhalf_scorer, n_iter = 100, cv = c_strat, verbose=2, \n",
    "                               random_state=rs, n_jobs = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_random.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_random.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_dt_random = dt_random.predict(X_test)"
   ]
  },
  {
   "source": [
    "### Grid Search"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the parameter grid based on the results of random search \n",
    "param_grid = {\n",
    "     'min_samples_split': [1, 2, 3, 4],\n",
    "     'min_samples_leaf': [1, 2, 3, 4],\n",
    "     'max_features': ['sqrt', 'auto'],\n",
    "     'max_depth': [5, 10, 15, 20]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the grid search model\n",
    "grid_search = GridSearchCV(estimator = dtree_clf_base, param_grid = param_grid, scoring=fhalf_scorer, \n",
    "                           cv = c_strat, n_jobs = -1, verbose = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the grid search to the data\n",
    "grid_search.fit(X_train, y_train)\n",
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_dt_grid = grid_search.predict(X_test)"
   ]
  },
  {
   "source": [
    "#### Evaluate Grid Search"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_pred_dt_grid))\n",
    "print(confusion_matrix(y_test, y_pred_dt_grid))\n",
    "print(f'F-Beta-Score(0.5) =  {fbeta_score(y_test, y_pred_dt_grid, beta=0.5 ):.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC Curve Random Search Model, Base Model and Dummy\n",
    "fpr_dt_gs, tpr_dt_gs, thresholds_dt_gs = roc_curve(y_test, y_pred_dt_grid)\n",
    "plt.plot(fpr, tpr, linestyle='--', label='Dummy')\n",
    "plt.plot(fpr_dt_bm, tpr_dt_bm, linestyle='-', label='DT Base Model')\n",
    "plt.plot(fpr_dt_gs, tpr_dt_gs, linestyle='-', label='RF Grid Search Model')\n",
    "plt.legend()\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.show()\n",
    "\n",
    "y_proba_dt_random = dt_random.predict_proba(X_test)\n",
    "print(f\"AUC(Dummy) = {roc_auc_score(y_test, y_pred_dumm_prob[:,1]):.2f}\")\n",
    "print(f\"AUC(BaseModel)= {roc_auc_score(y_test, y_proba_dtree_clf_base[:,1]):.2f}\")\n",
    "print(f\"AUC(GridSearch)= {roc_auc_score(y_test, y_proba_dt_random[:,1]):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "source": [
    "### Base Model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_clf_base = RandomForestClassifier(random_state=rs)\n",
    "rf_clf_base.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_rf_clf_base = rf_clf_base.predict(X_test)"
   ]
  },
  {
   "source": [
    "#### Evaluate Base Model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_pred_rf_clf_base))\n",
    "print(confusion_matrix(y_test, y_pred_rf_clf_base))\n",
    "print(f'F-Beta-Score(0.5) =  {fbeta_score(y_test, y_pred_rf_clf_base, beta=0.5 ):.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC Curve Base Model and Dummy\n",
    "fpr_rf_bm, tpr_rf_bm, thresholds_rf_bm = roc_curve(y_test, y_pred_rf_clf_base)\n",
    "plt.plot(fpr, tpr, linestyle='--', label='Dummy')\n",
    "plt.plot(fpr_rf_bm, tpr_rf_bm, linestyle='-', label='RF Base Model')\n",
    "plt.legend()\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.show()\n",
    "\n",
    "y_proba_rf_clf_base = rf_clf_base.predict_proba(X_test)\n",
    "print(f\"AUC(Dummy) = {roc_auc_score(y_test, y_pred_dumm_prob[:,1]):.2f}\")\n",
    "print(f\"AUC(BaseModel)= {roc_auc_score(y_test, y_proba_rf_clf_base[:,1]):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at parameters used by our current forest\n",
    "print('Parameters currently in use:\\n')\n",
    "pprint(rf_clf_base.get_params())"
   ]
  },
  {
   "source": [
    "### Random Search"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set scorer for random search to f-beta 0.5\n",
    "fhalf_scorer = make_scorer(fbeta_score, beta=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set cross-validation to a stratified split of 3\n",
    "c_strat=StratifiedKFold(n_splits=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of trees in random forest\n",
    "n_estimators = [int(x) for x in np.linspace(start = 100, stop = 2000, num = 10)]\n",
    "\n",
    "# Number of features to consider at every split\n",
    "max_features = ['auto', 'sqrt']\n",
    "\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
    "max_depth.append(None)\n",
    "\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 5, 10]\n",
    "\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "\n",
    "# Method of selecting samples for training each tree\n",
    "bootstrap = [True, False]\n",
    "\n",
    "# Create the random grid\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap}\n",
    "pprint(random_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random search of parameters, using 3 fold cross validation \n",
    "rf_random = RandomizedSearchCV(estimator = rf_clf_base, param_distributions = random_grid, scoring=fhalf_scorer, \n",
    "                               n_iter = 100, cv = c_strat, verbose=2, random_state=rs, n_jobs = -1)\n",
    "rf_random.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_random.best_params_"
   ]
  },
  {
   "source": [
    "### Grid Search"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the parameter grid based on the results of random search \n",
    "param_grid = {\n",
    "    'bootstrap': [True, False],\n",
    "    'max_depth': [10, 20, 30],\n",
    "    'max_features': ['sqrt', 'auto'],\n",
    "    'min_samples_leaf': [1, 2, 3],\n",
    "    'min_samples_split': [1, 2, 3, 4],\n",
    "    'n_estimators': [1000, 1500, 2000]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the grid search model\n",
    "grid_search = GridSearchCV(estimator = rf_clf_base, param_grid = param_grid, scoring=fhalf_scorer, \n",
    "                           cv = c_strat, n_jobs = -1, verbose = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the grid search to the data\n",
    "grid_search.fit(X_train, y_train)\n",
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_rf_clf_grid = grid_search.predict(X_test)"
   ]
  },
  {
   "source": [
    "#### Evaluate Grid Search"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_pred_rf_clf_grid))\n",
    "print(confusion_matrix(y_test, y_pred_rf_clf_grid))\n",
    "print(f'F-Beta-Score(0.5) =  {fbeta_score(y_test, y_pred_rf_clf_grid, beta=0.5 ):.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC Curve Random Search Model, Base Model and Dummy\n",
    "fpr_rf_gs, tpr_rf_gs, thresholds_rf_gs = roc_curve(y_test, y_pred_rf_clf_grid)\n",
    "plt.plot(fpr, tpr, linestyle='--', label='Dummy')\n",
    "plt.plot(fpr_rf_bm, tpr_rf_bm, linestyle='-', label='RF Base Model')\n",
    "plt.plot(fpr_rf_gs, tpr_rf_gs, linestyle='-', label='RF Grid Search Model')\n",
    "plt.legend()\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.show()\n",
    "\n",
    "y_proba_rf_clf_random = rf_random.predict_proba(X_test)\n",
    "print(f\"AUC(Dummy) = {roc_auc_score(y_test, y_pred_dumm_prob[:,1]):.2f}\")\n",
    "print(f\"AUC(BaseModel)= {roc_auc_score(y_test, y_proba_rf_clf_base[:,1]):.2f}\")\n",
    "print(f\"AUC(GridSearch)= {roc_auc_score(y_test, y_proba_rf_clf_random[:,1]):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instanciate classifier, fit model\n",
    "xgboost = XGBClassifier(random_state=rs)\n",
    "xgboost.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_xgb = xgboost.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_pred_xgb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fbeta_score(y_test, y_pred_xgb, beta=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test, xgboost.predict(X_test),)\n",
    "cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis] # normalize the data\n",
    "# view with a heatmap\n",
    "#plt.figure(i)\n",
    "sns.heatmap(cm, annot=True, annot_kws={\"size\":30}, \n",
    "            cmap='Blues', square=True, fmt='.3f')\n",
    "\n",
    "\n",
    "plt.ylabel('True label', fontsize=15)\n",
    "plt.xlabel('Predicted label', fontsize=15)\n",
    "plt.title('Confusion matrix', fontsize=15, fontweight=\"bold\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the ROC curve\n",
    "fpr_xgb, tpr_xgb, thresholds_xgb = roc_curve(y_test, y_pred_xgb)\n",
    "plt.plot(fpr, tpr, linestyle='--', label='Dummy')\n",
    "plt.plot(fpr_xgb, tpr_xgb, linestyle='-', label='Xgboost')\n",
    "plt.legend()\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.show()\n",
    "y_pred_xgb_prob = xgboost.predict_proba(X_test)\n",
    "print(f\"AUC(Dummy) = {roc_auc_score(y_test, y_pred_dumm_prob[:,1]):.2f}\")\n",
    "print(f\"AUC(Xgboost)= {roc_auc_score(y_test, y_pred_xgb_prob[:,1]):.2f}\")"
   ]
  },
  {
   "source": [
    "### random search"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_grid = dict()\n",
    "xgb_grid['max_depth'] = [2, 3, 4, 5, 6]\n",
    "xgb_grid['learning_rate'] = [0.01, 0.05, 0.1, 0.2, 0.4]\n",
    "xgb_grid['gamma'] = [0,0.1,0.2,0.3,0.4,0.5]\n",
    "\n",
    "model = XGBClassifier()\n",
    "grid = xgb_grid\n",
    "    \n",
    "# run randomized search and optimize for fhalf_scorer\n",
    "clf = RandomizedSearchCV(estimator=model,param_distributions=grid,n_iter = 10,verbose=1,\n",
    "                         scoring=fhalf_scorer, random_state=rs,cv=c)\n",
    "search = clf.fit(X_train, y_train)\n",
    "predictions = search.best_estimator_.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(search.best_params_)"
   ]
  },
  {
   "source": [
    "best result: {'max_depth': 3, 'learning_rate': 0.2, 'gamma': 0.5}"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_xgb_rdm = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fbeta_score(y_test, y_pred_xgb_rdm, beta=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test, clf.predict(X_test),)\n",
    "cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis] # normalize the data\n",
    "# view with a heatmap\n",
    "#plt.figure(i)\n",
    "sns.heatmap(cm, annot=True, annot_kws={\"size\":30}, \n",
    "            cmap='Blues', square=True, fmt='.3f')\n",
    "\n",
    "\n",
    "plt.ylabel('True label', fontsize=15)\n",
    "plt.xlabel('Predicted label', fontsize=15)\n",
    "plt.title('Confusion matrix', fontsize=15, fontweight=\"bold\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the ROC curve\n",
    "fpr_xgb, tpr_xgb, thresholds_lr = roc_curve(y_test, y_pred_xgb_rdm)\n",
    "plt.plot(fpr, tpr, linestyle='--', label='Dummy')\n",
    "plt.plot(fpr_xgb, tpr_xgb, linestyle='-', label='Xgboost')\n",
    "plt.legend()\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.show()\n",
    "y_pred_xgb_prob = clf.predict_proba(X_test)\n",
    "print(f\"AUC(Dummy) = {roc_auc_score(y_test, y_pred_dumm_prob[:,1]):.2f}\")\n",
    "print(f\"AUC(Xgboost)= {roc_auc_score(y_test, y_pred_xgb_prob[:,1]):.2f}\")"
   ]
  },
  {
   "source": [
    "### Gridsearch"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'max_depth': [2,3,4],\n",
    "    'learning_rate': [0.2, 0.15, 0,25],\n",
    "    'gamma': [0.5, 0.55, 0.45],}\n",
    "\n",
    "optimal_params = GridSearchCV(\n",
    "    estimator= XGBClassifier(), cv=c,\n",
    "    param_grid=param_grid,\n",
    "     scoring = fhalf_scorer)## For more scoring metics see: \n",
    "        ## https://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter\n",
    "\n",
    "optimal_params.fit(X_train, y_train)\n",
    "print(optimal_params.best_params_)"
   ]
  },
  {
   "source": [
    "{'gamma': 0.5, 'learning_rate': 0.2, 'max_depth': 4}"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_xgb_grid = optimal_params.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_pred_xgb_grid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fbeta_score(y_test, y_pred_xgb_grid, beta=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test, optimal_params.predict(X_test),)\n",
    "cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis] # normalize the data\n",
    "# view with a heatmap\n",
    "#plt.figure(i)\n",
    "sns.heatmap(cm, annot=True, annot_kws={\"size\":30}, \n",
    "            cmap='Blues', square=True, fmt='.3f')\n",
    "\n",
    "\n",
    "plt.ylabel('True label', fontsize=15)\n",
    "plt.xlabel('Predicted label', fontsize=15)\n",
    "plt.title('Confusion matrix', fontsize=15, fontweight=\"bold\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the ROC curve\n",
    "fpr_xgb, tpr_xgb, thresholds_xgb = roc_curve(y_test, y_pred_xgb_grid)\n",
    "plt.plot(fpr, tpr, linestyle='--', label='Dummy')\n",
    "plt.plot(fpr_xgb, tpr_xgb, linestyle='-', label='Xgboost_Grid')\n",
    "plt.legend()\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.show()\n",
    "y_pred_xgb_prob = optimal_params.predict_proba(X_test)\n",
    "print(f\"AUC(Dummy) = {roc_auc_score(y_test, y_pred_dumm_prob[:,1]):.2f}\")\n",
    "print(f\"AUC(Xgboost)= {roc_auc_score(y_test, y_pred_xgb_prob[:,1]):.2f}\")"
   ]
  },
  {
   "source": [
    "### best Xgboost model {'max_depth': 3, 'learning_rate': 0.2, 'gamma': 0.5}"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instanciate classifier, fit model\n",
    "xgboost_best = XGBClassifier(max_depth= 3, learning_rate= 0.2, gamma= 0.5, random_state=rs)\n",
    "xgboost_best.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_xgb_best = xgboost_best.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_pred_xgb_best))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fbeta_score(y_test, y_pred_xgb_best, beta=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test, y_pred_xgb_best)\n",
    "cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis] # normalize the data\n",
    "# view with a heatmap\n",
    "#plt.figure(i)\n",
    "sns.heatmap(cm, annot=True, annot_kws={\"size\":30}, \n",
    "            cmap='Blues', square=True, fmt='.3f')\n",
    "\n",
    "\n",
    "plt.ylabel('True label', fontsize=15)\n",
    "plt.xlabel('Predicted label', fontsize=15)\n",
    "plt.title('Confusion matrix', fontsize=15, fontweight=\"bold\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the ROC curve\n",
    "fpr_xgb_best, tpr_xgb_best, thresholds_xgb_best = roc_curve(y_test, y_pred_xgb_best)\n",
    "plt.plot(fpr, tpr, linestyle='--', label='Dummy')\n",
    "plt.plot(fpr_xgb_best, tpr_xgb_best, linestyle='-', label='Xgboost_best')\n",
    "plt.legend()\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.show()\n",
    "y_pred_xgb_best_prob = xgboost_best.predict_proba(X_test)\n",
    "print(f\"AUC(Dummy) = {roc_auc_score(y_test, y_pred_dumm_prob[:,1]):.2f}\")\n",
    "print(f\"AUC(Xgboost)= {roc_auc_score(y_test, y_pred_xgb_best_prob[:,1]):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ada = AdaBoostClassifier()\n",
    "ada.fit(X_train, y_train)\n",
    "y_pred_ada = ada.predict(X_test)\n",
    "print(classification_report(y_test, y_pred_ada))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fbeta_score(y_test, y_pred_ada, beta=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test, ada.predict(X_test),)\n",
    "cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis] # normalize the data\n",
    "# view with a heatmap\n",
    "#plt.figure(i)\n",
    "sns.heatmap(cm, annot=True, annot_kws={\"size\":30}, \n",
    "            cmap='Blues', square=True, fmt='.3f')\n",
    "\n",
    "\n",
    "plt.ylabel('True label', fontsize=15)\n",
    "plt.xlabel('Predicted label', fontsize=15)\n",
    "plt.title('Confusion matrix', fontsize=15, fontweight=\"bold\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the ROC curve\n",
    "fpr_ada, tpr_ada, thresholds_ada = roc_curve(y_test, y_pred)\n",
    "plt.plot(fpr, tpr, linestyle='--', label='Dummy')\n",
    "plt.plot(fpr_ada, tpr_ada, linestyle='-', label='Adaboost')\n",
    "plt.legend()\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.show()\n",
    "y_pred_ada_prob = ada.predict_proba(X_test)\n",
    "print(f\"AUC(Dummy) = {roc_auc_score(y_test, y_pred_dumm_prob[:,1]):.2f}\")\n",
    "print(f\"AUC(Adaboost)= {roc_auc_score(y_test, y_pred_ada_prob[:,1]):.2f}\")"
   ]
  },
  {
   "source": [
    "### Randomsearch"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_grid = dict()\n",
    "xgb_grid['max_depth'] = [2, 3, 4, 5, 6]\n",
    "xgb_grid['learning_rate'] = [0.01, 0.05, 0.1, 0.2, 0.4]\n",
    "xgb_grid['gamma'] = [0,0.1,0.2,0.3,0.4,0.5]\n",
    "\n",
    "model = AdaBoostClassifier()\n",
    "grid = xgb_grid\n",
    "    \n",
    "# run randomized search and optimize for fhalf_scorer\n",
    "clf = RandomizedSearchCV(estimator=model,param_distributions=grid,n_iter = 10,verbose=1,\n",
    "                         scoring=fhalf_scorer, random_state=rs, cv=c)\n",
    "search = ada_rdm.fit(X_train, y_train)\n",
    "predictions = search.best_estimator_.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(search.best_params_)"
   ]
  },
  {
   "source": [
    "{'learning_rate': 0.1, 'n_estimators': 500}"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_ada_rdm = ada_rdm.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_pred_ada_rdm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fbeta_score(y_test, y_pred_ada_rdm, beta=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test, ada_rdm.predict(X_test),)\n",
    "cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis] # normalize the data\n",
    "# view with a heatmap\n",
    "#plt.figure(i)\n",
    "sns.heatmap(cm, annot=True, annot_kws={\"size\":30}, \n",
    "            cmap='Blues', square=True, fmt='.3f')\n",
    "\n",
    "\n",
    "plt.ylabel('True label', fontsize=15)\n",
    "plt.xlabel('Predicted label', fontsize=15)\n",
    "plt.title('Confusion matrix', fontsize=15, fontweight=\"bold\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the ROC curve\n",
    "fpr_ada, tpr_ada, thresholds_ada = roc_curve(y_test, y_pred)\n",
    "plt.plot(fpr, tpr, linestyle='--', label='Dummy')\n",
    "plt.plot(fpr_ada, tpr_ada, linestyle='-', label='Adaboost')\n",
    "plt.legend()\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.show()\n",
    "y_pred_ada_prob = ada_rdm.predict_proba(X_test)\n",
    "print(f\"AUC(Dummy) = {roc_auc_score(y_test, y_pred_dumm_prob[:,1]):.2f}\")\n",
    "print(f\"AUC(Adaboost)= {roc_auc_score(y_test, y_pred_ada_prob[:,1]):.2f}\")"
   ]
  },
  {
   "source": [
    "### Gridsearch"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Import 'GridSearchCV', 'make_scorer', and any other necessary libraries\n",
    "\n",
    "# TODO: Initialize the classifier\n",
    "ada = AdaBoostClassifier(base_estimator=DecisionTreeClassifier()) \n",
    "\n",
    "# TODO: Create the parameters list you wish to tune\n",
    "parameters = {'n_estimators':[500],                \n",
    "              'learning_rate':[0.1, 0.5, 0.15,],               \n",
    "              'base_estimator__min_samples_split' : np.arange(2, 8, 2),               \n",
    "              'base_estimator__max_depth' : np.arange(1, 4, 1)              \n",
    "             } \n",
    "\n",
    "# TODO: Make an fbeta_score scoring object\n",
    "scorer = fhalf_scorer\n",
    "\n",
    "# TODO: Perform grid search on the classifier using 'scorer' as the scoring method\n",
    "grid_obj = GridSearchCV(ada, parameters,scorer, cv=c) \n",
    "\n",
    "# TODO: Fit the grid search object to the training data and find the optimal parameters\n",
    "grid_fit = grid_obj.fit(X_train,y_train) \n",
    "\n",
    "# Get the estimator\n",
    "best_ada = grid_fit.best_estimator_ \n",
    "\n",
    "# Make predictions using the unoptimized and model\n",
    "predictions = (ada.fit(X_train, y_train)).predict(X_test)\n",
    "best_predictions = best_ada.predict(X_test) \n",
    "\n",
    "# Report the before-and-afterscores\n",
    "print(\"Unoptimized model\\n------\")\n",
    "print(\"Accuracy score on testing data: {:.4f}\".format(accuracy_score(y_test, predictions)))\n",
    "print(\"F-score on testing data: {:.4f}\".format(fbeta_score(y_test, predictions, beta = 0.5)))\n",
    "print(\"\\nOptimized Model\\n------\")\n",
    "print(\"Final accuracy score on the testing data: {:.4f}\".format(accuracy_score(y_test, best_predictions)))\n",
    "print(\"Final F-score on the testing data: {:.4f}\".format(fbeta_score(y_test, best_predictions, beta = 0.5)))\n",
    "print(best_ada)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ada_grid = AdaBoostClassifier(base_estimator=DecisionTreeClassifier\n",
    "                              (max_depth=2,\n",
    "                               min_samples_split=6),\n",
    "                              learning_rate=0.1, \n",
    "                              n_estimators=500)\n",
    "ada_grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_ada_grid = ada_grid.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_pred_ada_grid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fbeta_score(y_test, y_pred_ada_grid, beta=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test, ada_grid.predict(X_test),)\n",
    "cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis] # normalize the data\n",
    "# view with a heatmap\n",
    "#plt.figure(i)\n",
    "sns.heatmap(cm, annot=True, annot_kws={\"size\":30}, \n",
    "            cmap='Blues', square=True, fmt='.3f')\n",
    "\n",
    "\n",
    "plt.ylabel('True label', fontsize=15)\n",
    "plt.xlabel('Predicted label', fontsize=15)\n",
    "plt.title('Confusion matrix', fontsize=15, fontweight=\"bold\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the ROC curve\n",
    "fpr_ada, tpr_ada, thresholds_ada = roc_curve(y_test, y_pred)\n",
    "plt.plot(fpr, tpr, linestyle='--', label='Dummy')\n",
    "plt.plot(fpr_ada, tpr_ada, linestyle='-', label='Adaboost')\n",
    "plt.legend()\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.show()\n",
    "y_pred_ada_prob = ada_grid.predict_proba(X_test)\n",
    "print(f\"AUC(Dummy) = {roc_auc_score(y_test, y_pred_dumm_prob[:,1]):.2f}\")\n",
    "print(f\"AUC(Adaboost)= {roc_auc_score(y_test, y_pred_ada_prob[:,1]):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_model = SVC(C=1, kernel='rbf', probability=True, random_state=rs)\n",
    "svc = svc_model.fit(X_train, y_train)\n",
    "y_pred_svc = svc.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_pred_svc))\n",
    "print(confusion_matrix(y_test, y_pred_svc))\n",
    "print(f'F-Beta-Score(0.5) =  {fbeta_score(y_test, y_pred_svc, beta=0.5 ):.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_svc_prob = svc.predict_proba(X_test)\n",
    "print(f'ROC-AUC-Score = {roc_auc_score(y_test, y_pred_svc_prob[:,1]):.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred_svc)\n",
    "auc_score = roc_auc_score(y_test, y_pred_svc)\n",
    "\n",
    "plt.figure(figsize=(10,7))\n",
    "plt.plot(fpr, tpr, linewidth=2) \n",
    "plt.plot([0, 1], [0, 1], 'k--') \n",
    "plt.ylabel('True Positive Rate (Recall)')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.title('ROC Curve for Base SVC', loc='left')\n",
    "plt.title(f'AUC Score: {auc_score:.3f}', loc='right');"
   ]
  },
  {
   "source": [
    "### Optimize Model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_svc = {'C': [0.1,1, 10, 50, 100],\n",
    "             'kernel': ['linear', 'rbf', 'poly','sigmoid'],\n",
    "             'gamma' : [1, 0.1, 0.01, 0.001]\n",
    "             }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_grid = GridSearchCV(SVC(), \n",
    "                        param_grid=grid_svc, \n",
    "                        cv=c, \n",
    "                        verbose=False, n_jobs=-1,\n",
    "                        probability=True, random_state=rs,\n",
    "                        scoring=fhalf_scorer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_grid.fit(X_train, y_train)\n",
    "y_pred_svc_grid = svc_grid.predict(X_test)\n",
    "svc_grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_pred_svc_grid))\n",
    "print(confusion_matrix(y_test, y_y_pred_svc_gridpred))\n",
    "print(f'F-Beta-Score(0.5) =  {fbeta_score(y_test, y_pred_svc_grid, beta=0.5 ):.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame(svc_grid.cv_results_)\n",
    "results.sort_values('rank_test_score').head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## kNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple kNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_accuracy = []\n",
    "test_accuracy = []\n",
    "train_beta = []\n",
    "test_beta = []\n",
    "\n",
    "# Probiere Werte f√ºr k von 1 bis 10 aus\n",
    "neighbor_settings = range(1, 20)\n",
    "\n",
    "for k in neighbor_settings:\n",
    "    clf = KNeighborsClassifier(n_neighbors=k)\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    train_beta.append(fbeta_score(y_test, y_pred, beta=0.5))\n",
    "    train_accuracy.append(clf.score(X_train, y_train))\n",
    "    test_accuracy.append(clf.score(X_test, y_test))\n",
    "\n",
    "plt.plot(neighbor_settings, train_accuracy, label='Accuracy Training')\n",
    "plt.plot(neighbor_settings, test_accuracy, label='Accuracy Test')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Number neighbors')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(neighbor_settings, train_beta)\n",
    "plt.ylabel('F-beta-score')\n",
    "plt.xlabel('Number neighbors')\n",
    "plt.xticks(ticks=list(range(0,21,2)) ,labels=list(range(1,21,2)));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We take 17 neighbours as best value, for the first simple model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=17)\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred_knn= knn.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_pred_knn))\n",
    "print(confusion_matrix(y_test, y_pred_knn))\n",
    "print(f'F-Beta-Score(0.5) =  {fbeta_score(y_test, y_pred_knn, beta=0.5 ):.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_knn_prob = clf.predict_proba(X_test)\n",
    "print(f'ROC-AUC-Score = {roc_auc_score(y_test, y_pred_knn_prob[:,1]):.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred_knn)\n",
    "auc_score = roc_auc_score(y_test, y_pred_knn)\n",
    "\n",
    "plt.figure(figsize=(10,7))\n",
    "plt.plot(fpr, tpr, linewidth=2) \n",
    "plt.plot([0, 1], [0, 1], 'k--') \n",
    "plt.ylabel('True Positive Rate (Recall)')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.title('ROC Curve for Base KNN', loc='left')\n",
    "plt.title(f'AUC Score: {auc_score}', loc='right');"
   ]
  },
  {
   "source": [
    "### Optimize Model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_knn = {'n_neighbors' : list(range(1,30)),\n",
    "             'weights': ['uniform', 'distance'],\n",
    "             'leaf_size' : list(range(1,20)),\n",
    "             'metric' : ['euclidean','minkowski','manhattan']\n",
    "             }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_grid_mod = GridSearchCV(KNeighborsClassifier(), \n",
    "                        param_grid=grid_knn, \n",
    "                        cv=c, \n",
    "                        verbose=False, n_jobs=-1,\n",
    "                        scoring=fhalf_scorer)\n",
    "knn_grid_mod.fit(X_train, y_train)\n",
    "y_pred = knn_grid_mod.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(f'F-Beta-Score(0.5) =  {fbeta_score(y_test, y_pred, beta=0.5 ):.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_grid_mod.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_prob = knn_grid_mod.predict_proba(X_test)\n",
    "print(f'ROC-AUC-Score = {roc_auc_score(y_test, y_pred_prob[:,1]):.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes"
   ]
  },
  {
   "source": [
    "### Hyperparameter by default"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gnb = GaussianNB()\n",
    "y_pred_gnb = gnb.fit(X_train, y_train).predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_pred_gnb))\n",
    "print(f'F-Beta-Score(0.5) =  {fbeta_score(y_test, y_pred_gnb, beta=0.5 ):.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot non-normalized confusion matrix\n",
    "cnf_matrix = confusion_matrix(y_test, y_pred_gnb)\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix, classes=['Failed','Succeeded'],\n",
    "                      normalize= False,  title='Confusion matrix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the ROC curve\n",
    "fpr_gnb, tpr_gnb, thresholds_gnb = roc_curve(y_test, y_pred_gnb)\n",
    "plt.plot(fpr, tpr, linestyle='--', label='Dummy')\n",
    "plt.plot(fpr_gnb, tpr_gnb, linestyle='-', label='Gaussian Naive Bayes') \n",
    "plt.legend()\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.show()\n",
    "y_pred_gnb_prob = gnb.predict_proba(X_test)\n",
    "print(f\"AUC(Dummy) = {roc_auc_score(y_test, y_pred_dumm_prob[:,1]):.2f}\")\n",
    "print(f\"AUC(GNB)= {roc_auc_score(y_test, y_pred_gnb_prob[:,1]):.2f}\")"
   ]
  },
  {
   "source": [
    "### Hyperparameter Tuning"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter grid for GridSearch for Naive Bayes\n",
    "params_grid = {'var_smoothing': np.logspace(1,-9, num=1000)}\n",
    "\n",
    "# Estimator for use in gridSearch\n",
    "estimator = GaussianNB()\n",
    "\n",
    "# Create the gridSearch model\n",
    "gnb_gs = GridSearchCV(estimator, params_grid, scoring= fhalf_scorer, cv = c,\n",
    "                     n_jobs = -1, verbose = 1)\n",
    "\n",
    "# Fit \n",
    "gnb_gs.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gnb_gs.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_gnb_gs = gnb_gs.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_gnb_best = best_model_gnb_gs.predict(X_test)\n",
    "print(classification_report(y_test, y_pred_gnb_best))\n",
    "print(f'F-Beta-Score(0.5) =  {fbeta_score(y_test, y_pred_gnb_best, beta=0.5 ):.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot non-normalized confusion matrix\n",
    "cnf_matrix = confusion_matrix(y_test, y_pred_gnb_best)\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix, classes=['Failed','Succeeded'],\n",
    "                      normalize= False,  title='Confusion matrix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the ROC curve\n",
    "fpr_gnb_best, tpr_gnb_best, thresholds_gnb_best = roc_curve(y_test, y_pred_gnb_best)\n",
    "plt.plot(fpr, tpr, linestyle='--', label='Dummy')\n",
    "plt.plot(fpr_gnb, tpr_gnb, linestyle='-', label='Naive Bayes') \n",
    "plt.plot(fpr_gnb_best, tpr_gnb_best, linestyle=':', label='Best Naive Bayes')\n",
    "plt.legend()\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.show()\n",
    "y_pred_gnb_best_prob = best_model_gnb_gs.predict_proba(X_test)\n",
    "print(f\"AUC(Dummy) = {roc_auc_score(y_test, y_pred_dumm_prob[:,1]):.2f}\")\n",
    "print(f\"AUC(GNB)= {roc_auc_score(y_test, y_pred_gnb_prob[:,1]):.2f}\")\n",
    "print(f\"AUC(BestGNB)= {roc_auc_score(y_test, y_pred_gnb_best_prob[:,1]):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.6.10 64-bit ('nf': conda)",
   "metadata": {
    "interpreter": {
     "hash": "655c38a0198c9c2039048b9991a18991b95596824c143f97329cca90952da2be"
    }
   }
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10-final"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}